\subsection{Research on related software}
\subsubsection{LG ThinQ}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
LG ThinQ is an integrated smart home platform that connects and controls all LG appliances within the household through a centralized hub architecture. It provides the highest level of interoperability and system stability within the LG ecosystem. 

In particular, the ThinQ API enables precise monitoring and control of appliance states such as those of washing machines, air conditioners, and air purifiers. Furthermore, the UP System adopts a modular design that supports continuous enhancement of appliance functionality through over-the-air (OTA) updates, allowing devices to evolve even after deployment.

These technical features constitute a core operational foundation for this project, enabling the behavioral policies generated by the AI persona to be executed reliably at the appliance level. In other words, LG ThinQ serves as the most essential interface that ensures the accurate and dependable realization of the persona’s decisions within the physical smart home environment.
\end{adjustwidth}

\subsubsection{Samsung SmartThings}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Samsung SmartThings is a representative multi-brand smart home platform that enables users to control devices from various manufacturers within a unified application environment. At the hub level, it employs Edge Driver technology to support local device control, thereby minimizing network latency and instability while reducing dependence on cloud processing. 

In addition, its Energy Management System provides real-time visualization and control of power consumption for individual appliances, achieving a high level of completeness in terms of energy efficiency management.

This high degree of interoperability and data integration capability makes SmartThings particularly effective for this project, which aims to manage and unify appliances from multiple brands under a single policy framework. 

Moreover, since the SmartThings ecosystem is natively integrated with the Matter standard, it provides a strong infrastructural foundation for expanding the proposed system’s policies and control mechanisms into a universally compatible smart home environment.
\end{adjustwidth}

\subsubsection{Google Home (Assistant)}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Google Home is Google’s smart home platform, developed through the integration of Nest devices and Chromecast, and is characterized by its deep connectivity with the broader Google ecosystem—including Google Account, Calendar, and Maps. Through this contextual integration, the platform can directly incorporate the user’s schedule, location, and habitual behaviors into automated routines and scenario generation.

In particular, Google Assistant’s advanced speech recognition and natural language processing (NLP) technologies have reached a level where the system can not only interpret explicit commands but also understand conversational expressions in context and translate them into corresponding automated actions. 

This natural language–based automation mechanism closely aligns with the conversational engine structure of our proposed system, serving as a direct reference for designing models that convert dialogue-based instructions into policy-level inputs. Furthermore, by linking with calendar and location data, Google Home can provide valuable contextual and emotional cues, enhancing the system’s ability to accurately perceive the user’s current state and situational context.
\end{adjustwidth}

\subsubsection{openHAB}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
openHAB is an open-source home automation hub designed to minimize dependence on cloud infrastructure, specializing in local control and on-premise automation. Because it can be executed directly on a user’s own server or single-board computer, openHAB allows data to be securely stored within the household, offering strong advantages in terms of data privacy and user sovereignty. 

At the same time, it maintains compatibility with major cloud-based platforms such as Google Assistant, Amazon Alexa, and Apple HomeKit, thereby supporting a flexible hybrid operating environment.

Currently, openHAB provides integration with over 3,000 global brands and devices, including Qualcomm AllPlay, Android TV, BenQ, and AIRTHINGS. In addition to its extensive interoperability, the platform enables local learning and inference, making it a valuable experimental base for testing reinforcement learning (RL) models or persona-driven policy frameworks within a controlled, privacy-preserving environment. 

This combination of openness, scalability, and on-device intelligence positions openHAB as a practical research platform for developing and validating next-generation smart home systems.
\end{adjustwidth}

\subsubsection{Apple HomeKit}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Apple Home is a smart home platform deeply integrated with the iOS ecosystem, built upon an architecture that prioritizes security and privacy above all else. Through the HomeKit protocol, only certified and authenticated devices are permitted to connect to the network, ensuring a highly reliable and secure environment. 

More recently, Apple Home has adopted support for the Matter standard and Thread networking, significantly enhancing the stability and interoperability of low-power devices such as sensors and lighting systems.

In addition, Apple’s characteristic consistent iOS user experience (UX) and the seamless integration of the Siri voice assistant provide users with high accessibility and an intuitive automation setup process. These features directly align with our project’s design philosophy, which emphasizes on-device learning and local data protection. 

In particular, HomeKit’s local inference architecture serves as a concrete reference model for developing the project’s on-device persona intelligence strategy, where personalized behaviors and contextual reasoning are executed securely within the user’s own environment.
\end{adjustwidth}

\subsubsection{Amazon Alexa}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Amazon Alexa is a voice-assistant platform built around smart speaker technology and is currently one of the most widely adopted voice interfaces for smart home ecosystems. Alexa offers thousands of extensions (Skills) and automation scenarios (Routines) that enable a broad range of functions, including appliance control, music playback, and information retrieval. 

In recent developments, Alexa has integrated large language model (LLM)-based natural language routine generation, allowing users to configure automation simply through conversational expressions such as “When this happens, do that.”

This advancement in voice user experience (UX) and affective language processing represents one of the most mature commercial implementations of an emotionally responsive home environment. In this regard, Alexa serves as a practical reference for the vision of our system—a home that understands and responds to the user’s emotions and contextual needs, bridging natural conversation with intelligent, adaptive automation.
\end{adjustwidth}

\subsubsection{Matter}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Matter is a global standard protocol designed to ensure interoperability among smart home devices, allowing them to operate seamlessly regardless of brand or platform. 

By utilizing Thread networking technology, Matter enables mesh communication among low-power devices such as sensors and switches, while its simplified commissioning process enhances both the scalability of smart home environments and the consistency of user experience.

For the proposed system, Matter serves as a foundational technological framework that ensures the persistence and portability of learned behaviors. When the AI persona generates automation scenarios based on its learned behavioral policies, Matter allows these policies to be transferred and maintained across newly added or replaced devices. 

This interoperability provides the essential infrastructure for realizing a continuously evolving and universally compatible intelligent home environment.
\end{adjustwidth}

\subsubsection{Hume AI}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Hume AI is an affective artificial intelligence research company that quantifies users’ emotional states by analyzing subtle variations in vocal tone, speed, intensity, and prosody. Beyond simple positive–negative sentiment classification, Hume AI’s models identify underlying vocal patterns to estimate complex emotional indicators such as stress, fatigue, and arousal levels. 

Furthermore, the company’s technology can infer emotional states using microphone input alone, without requiring visual data from cameras, making it particularly effective and privacy-compliant in environments where visual information collection is sensitive or restricted.
\end{adjustwidth}

\subsubsection{Replika}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Replika is an emotionally adaptive chatbot that continuously learns from users’ conversational data to develop individualized personalities and emotional responses. By analyzing each user’s linguistic patterns, emotional expressions, and conversational history, Replika maintains relational consistency while gradually evolving its persona over time. 

This architecture provides a direct conceptual foundation for the design of smart home agents that move beyond simple command execution—enabling systems that continuously optimize their behavioral policies and response styles through ongoing user feedback and long-term interaction.
\end{adjustwidth}

\subsubsection{Affectiva}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Affectiva is a pioneer in emotion recognition (Emotion AI) technology that analyzes facial expressions and vocal cues to interpret human emotions with high precision. 

Its research and commercial applications span multiple industries—including automotive systems, robotics, and advertising analytics—focusing on designing interactive systems that adapt their responses based on users’ emotional states. 

This approach offers valuable insights for the development of multimodal persona architectures in smart home agents, enabling them to interpret and respond to emotional signals not only from voice data but also from visual inputs, thereby enhancing the depth and empathy of human–AI interaction.
\end{adjustwidth}

\subsubsection{Twilio}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Twilio provides a comprehensive cloud-based voice communication infrastructure through its Programmable Voice API, enabling applications to initiate, receive, and manage phone calls programmatically. 

The platform supports multiple communication protocols, including WebRTC, SIP (Session Initiation Protocol), and PSTN (Public Switched Telephone Network), allowing the implementation of reliable bidirectional voice communication between the home AI system and the user—whether the user calls the home AI or the home AI initiates a call to the user.

Twilio’s API further supports real-time speech recognition (STT), DTMF input detection, and IVR (Interactive Voice Response) flow control, while the use of TwiML (Twilio Markup Language) allows developers to model detailed call scenarios directly at the code level. 

When integrated with the Model Context Protocol (MCP), this architecture enables the AI model to dynamically manage call flows, process real-time call events as contextual inputs, and perform conversational decision-making within an ongoing voice interaction.
\end{adjustwidth}

\subsubsection{Vonage}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Vonage is a global communication platform centered on its Voice API, supporting bidirectional voice streaming through WebRTC and SIP-based protocols. The Vonage API allows real-time transmission of audio streams to external AI endpoints during a call, making it highly suitable for AI-integrated call processing tasks such as speech recognition, emotion analysis, and intent understanding. 

In particular, the Voice Connectors feature enables direct linkage with the Model Context Protocol (MCP) via a WebSocket interface, allowing the AI model to interpret user utterances in real time and generate immediate, context-aware responses during an ongoing conversation.

For instance, in a Vonage Voice API–enabled call, if the user’s voice tone rises, the MCP can recognize this as a change in emotional state, triggering an emergency response mode, or prompting the conversational persona to respond in a calmer, more empathetic tone. 

In this way, Vonage transforms call audio from a mere sound stream into a contextual communication medium, providing a crucial technological foundation for implementing the persona-based bidirectional communication framework proposed in this project.
\end{adjustwidth}

\subsubsection{Google Maps Timeline}
\begin{adjustwidth}{1em}{0em}
\setlength{\parindent}{1em} 
Google Maps Timeline is a location-tracking service that records users’ movement paths, visited places, transportation modes, dwell times, and travel distances based on a combination of GPS and Wi-Fi signal data. By integrating additional contextual information from Google’s web and app activity as well as photo metadata, the system constructs a comprehensive log of the user’s daily mobility and behavioral patterns.

This multi-source dataset provides valuable contextual signals that, when combined with biometric and weather data, significantly enhance the accuracy of lifestyle and condition analysis models—enabling fine-grained estimation of user activity levels, fatigue, and rest cycles.

Within the proposed system, the location and activity data from Google Maps Timeline are incorporated as contextual inputs for adaptive decision-making. For instance, if the system detects higher-than-usual mobility or a delayed return home, it can infer potential fatigue and proactively suggest personalized environmental adjustments, such as “Would you like to dim the lights?” or “Shall I activate sleep mode?” 

To ensure privacy, raw location data are anonymized and aggregated into grid-based representations stored locally on the device, while users maintain full control over the scope and duration of timeline recording and sharing preferences.
\end{adjustwidth}