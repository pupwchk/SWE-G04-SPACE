============================================================
FATIGUE PREDICTION MODEL - EVALUATION SUMMARY (3-Class)
============================================================

Model: XGBoost Classifier
Classes: Low (1-2), Medium (3), High (4-5)
CV Strategy: Leave-One-Participant-Out (16 folds)
Total Samples: 1729
Features: 7 Apple Watch compatible

============================================================
OVERALL PERFORMANCE
============================================================

Accuracy:       0.5626 ± 0.1540
F1 (Macro):     0.3261 ± 0.0954
F1 (Weighted):  0.5140 ± 0.1942

============================================================
CLASSIFICATION REPORT
============================================================

              precision    recall  f1-score   support

         Low       0.40      0.25      0.31       569
      Medium       0.61      0.81      0.70      1032
        High       0.33      0.02      0.03       128

    accuracy                           0.57      1729
   macro avg       0.45      0.36      0.35      1729
weighted avg       0.52      0.57      0.52      1729

============================================================
PARTICIPANT-WISE RESULTS
============================================================

participant  n_samples  accuracy  f1_macro  f1_weighted
        p01        138  0.565217  0.240741     0.408213
        p02         99  0.828283  0.338713     0.879848
        p03         78  0.846154  0.356446     0.845168
        p04        144  0.555556  0.288290     0.443739
        p05        134  0.716418  0.335536     0.667169
        p06        147  0.632653  0.315924     0.569055
        p07        132  0.643939  0.328345     0.650092
        p08        103  0.271845  0.175741     0.179366
        p09        102  0.480392  0.350361     0.412189
        p10         95  0.421053  0.285050     0.421414
        p11        107  0.635514  0.608720     0.667093
        p12         83  0.469880  0.274854     0.359755
        p13         68  0.397059  0.227670     0.264716
        p14        124  0.500000  0.399074     0.493840
        p15         93  0.451613  0.305187     0.385409
        p16         82  0.585366  0.387632     0.576426