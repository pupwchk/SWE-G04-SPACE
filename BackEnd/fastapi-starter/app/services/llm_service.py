"""
LLM ì„œë¹„ìŠ¤ - OpenAI APIë¥¼ ì‚¬ìš©í•œ AI ì‘ë‹µ ìƒì„±
"""
import os
import json
import logging
from typing import Dict, Any, Optional, List
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY", ""))


class LLMAction:
    """LLM ì•¡ì…˜ íƒ€ì…"""
    NONE = "NONE"  # ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ
    CALL = "CALL"  # ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì „í™”
    AUTO_CALL = "AUTO_CALL"  # GPS ê¸°ë°˜ ìë™ ì „í™”


class LLMService:
    """LLM ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.model = os.getenv("OPENAI_MODEL", "gpt-4o")
    
    def _build_system_prompt(self, persona: Optional[Dict] = None) -> str:
        """
        ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            persona: í˜ë¥´ì†Œë‚˜ ì •ë³´ (ë§íˆ¬, ì„±ê²© ë“±)
        """
        base_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìŠ¤ë§ˆíŠ¸í™ˆ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì—­í• :**
- ì‚¬ìš©ìì™€ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”
- ì§‘ì•ˆì¼ ë„ì›€ (ê°€ì „ì œí’ˆ ì œì–´, ì¼ì • ê´€ë¦¬ ë“±)
- ì‚¬ìš©ìì˜ ìƒíƒœ íŒŒì•… (í”¼ë¡œë„, ìŠ¤íŠ¸ë ˆìŠ¤ ë“±)
- í•„ìš”ì‹œ ì „í™”ë¡œ ì§ì ‘ ëŒ€í™” ì œì•ˆ

**ì‘ë‹µ í˜•ì‹:**
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:

1. ì¼ë°˜ í…ìŠ¤íŠ¸ ì‘ë‹µ:
{
  "action": "NONE",
  "response": "ì‘ë‹µ ë©”ì‹œì§€"
}

2. ì „í™” ê±¸ê¸° (ì‚¬ìš©ìê°€ ìš”ì²­í–ˆì„ ë•Œ):
{
  "action": "CALL",
  "response": "ì „í™” ë“œë¦´ê²Œìš”!",
  "reason": "ì‚¬ìš©ì ìš”ì²­"
}

3. GPS ê¸°ë°˜ ìë™ ì „í™” (ì§‘ ê·¼ì²˜ ë„ì°© ì‹œ):
{
  "action": "AUTO_CALL",
  "trigger": "GEO_FENCE",
  "response": "ì§‘ì— ê±°ì˜ ë‹¤ ì˜¤ì…¨ë„¤ìš”. í•„ìš”í•œ ê²Œ ìˆì„ê¹Œìš”?",
  "message_to_user": "ì ì‹œ í›„ ì „í™” ë“œë¦´ê²Œìš”."
}

**ì¤‘ìš”:**
- í•­ìƒ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”
- ì‚¬ìš©ìê°€ "ì „í™”í•´ì¤˜", "í†µí™”í•˜ì" ë“±ì„ ë§í•˜ë©´ action: "CALL"
- ì¼ìƒì ì¸ ëŒ€í™”ëŠ” action: "NONE"
"""
        
        if persona:
            base_prompt += f"\n**ë§íˆ¬/ì„±ê²©:**\n{persona.get('description', '')}\n"
        
        return base_prompt
    
    async def generate_response(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict]] = None,
        persona: Optional[Dict] = None,
        context: Optional[Dict] = None,
        appliance_states: Optional[List[Dict]] = None,
        dialogue_state: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        LLM ì‘ë‹µ ìƒì„±

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            persona: í˜ë¥´ì†Œë‚˜ ì •ë³´
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ (ìœ„ì¹˜, ì‹œê°„, ìƒíƒœ ë“±)
            appliance_states: í˜„ì¬ ê°€ì „ ìƒíƒœ ë¦¬ìŠ¤íŠ¸
            dialogue_state: DST ìƒíƒœ (intent, slots)

        Returns:
            {
                "action": "NONE" | "CALL" | "AUTO_CALL",
                "response": "ì‘ë‹µ ë©”ì‹œì§€",
                ...
            }
        """
        try:
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {"role": "system", "content": self._build_system_prompt(persona)}
            ]

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            if conversation_history:
                messages.extend(conversation_history[-10:])  # ìµœê·¼ 10ê°œë§Œ

            # DST ìƒíƒœ ì¶”ê°€ (í˜„ì¬ ëŒ€í™” ë§¥ë½)
            if dialogue_state and (dialogue_state.get("intent") or dialogue_state.get("slots")):
                dst_str = f"\n\n**ëŒ€í™” ë§¥ë½ (DST):**\n"
                if dialogue_state.get("intent"):
                    dst_str += f"- í˜„ì¬ ì˜ë„: {dialogue_state['intent']}\n"
                if dialogue_state.get("slots"):
                    dst_str += f"- ëŒ€í™” ì¤‘ì¸ ì£¼ì œ: {json.dumps(dialogue_state['slots'], ensure_ascii=False)}\n"
                messages.append({"role": "system", "content": dst_str})

            # ê°€ì „ ìƒíƒœ ì¶”ê°€
            if appliance_states:
                appliance_str = "\n\n**í˜„ì¬ ê°€ì „ ìƒíƒœ:**\n"
                for app in appliance_states:
                    status = "ì¼œì§" if app.get("is_on") else "êº¼ì§"
                    appliance_str += f"- {app['appliance_type']}: {status}"
                    if app.get("is_on") and app.get("current_settings"):
                        appliance_str += f" (ì„¤ì •: {json.dumps(app['current_settings'], ensure_ascii=False)})"
                    appliance_str += "\n"
                messages.append({"role": "system", "content": appliance_str})

            # ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
            if context:
                context_str = f"\n\n**í˜„ì¬ ìƒí™©:**\n{json.dumps(context, ensure_ascii=False, indent=2)}"
                messages.append({"role": "system", "content": context_str})

            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            messages.append({"role": "user", "content": user_message})
            
            # OpenAI API í˜¸ì¶œ
            response = await client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            # ì‘ë‹µ íŒŒì‹±
            content = response.choices[0].message.content
            result = json.loads(content)
            
            logger.info(f"âœ… LLM response: action={result.get('action')}")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse LLM response: {e}")
            # Fallback
            return {
                "action": "NONE",
                "response": "ì£„ì†¡í•´ìš”, ì˜ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
            }
        except Exception as e:
            logger.error(f"âŒ LLM error: {str(e)}")
            return {
                "action": "NONE",
                "response": "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            }
    
    async def generate_geofence_trigger(
        self,
        user_id: str,
        distance: float,
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        Geofence ì§„ì… ì‹œ ìë™ ì „í™” íŠ¸ë¦¬ê±° ìƒì„±
        
        Args:
            user_id: ì‚¬ìš©ì ID
            distance: ì§‘ê¹Œì§€ ê±°ë¦¬ (ë¯¸í„°)
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
        
        Returns:
            AUTO_CALL ì•¡ì…˜
        """
        try:
            prompt = f"""ì‚¬ìš©ìê°€ ì§‘ì—ì„œ {distance:.0f}m ê±°ë¦¬ì— ìˆìŠµë‹ˆë‹¤.
ì§‘ì— ê±°ì˜ ë„ì°©í–ˆìœ¼ë¯€ë¡œ ìë™ìœ¼ë¡œ ì „í™”ë¥¼ ê±¸ì–´ í•„ìš”í•œ ê²ƒì´ ìˆëŠ”ì§€ ë¬¼ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "action": "AUTO_CALL",
  "trigger": "GEO_FENCE",
  "response": "ì§‘ì— ë„ì°©í•˜ê¸° ì „ì— ì „í™”ë¡œ í•„ìš”í•œ ê²ƒì„ ë¬¼ì–´ë³¼ ë©”ì‹œì§€",
  "message_to_user": "ì „í™” ê±¸ê¸° ì „ì— ì±„íŒ…ìœ¼ë¡œ ë³´ë‚¼ ì§§ì€ ë©”ì‹œì§€"
}}
"""
            
            if context:
                prompt += f"\n\nì¶”ê°€ ì •ë³´:\n{json.dumps(context, ensure_ascii=False, indent=2)}"
            
            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._build_system_prompt()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            result = json.loads(content)
            
            logger.info(f"âœ… Geofence trigger generated for user {user_id}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Geofence trigger error: {str(e)}")
            # Fallback
            return {
                "action": "AUTO_CALL",
                "trigger": "GEO_FENCE",
                "response": "ì§‘ì— ê±°ì˜ ë‹¤ ì˜¤ì…¨ë„¤ìš”. í•„ìš”í•œ ê²Œ ìˆì„ê¹Œìš”?",
                "message_to_user": "ì§‘ì— ê±°ì˜ ë„ì°©í•˜ì…¨ì–´ìš”. ì ì‹œ í›„ ì „í™” ë“œë¦´ê²Œìš”."
            }

    async def parse_user_intent(self, user_message: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì˜ë„ íŒŒì‹± (ì‹œë‚˜ë¦¬ì˜¤ 2ìš©)

        "ë¥ë‹¤" â†’ {"type": "temperature", "condition": "hot"}
        "ê±´ì¡°í•˜ë‹¤" â†’ {"type": "humidity", "condition": "dry"}
        "ê³µê¸° ë‚˜ì˜ë‹¤" â†’ {"type": "air_quality", "condition": "bad"}

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            context: í˜„ì¬ ë‚ ì”¨/ìƒíƒœ ì •ë³´

        Returns:
            {
                "intent_type": "environment_complaint" | "general_chat" | "appliance_request",
                "issues": [
                    {"type": "temperature", "condition": "hot"},
                    {"type": "humidity", "condition": "dry"}
                ],
                "needs_control": true/false
            }
        """
        try:
            prompt = f"""ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "intent_type": "environment_complaint" | "general_chat" | "appliance_request",
  "issues": [
    {{"type": "temperature|humidity|air_quality", "condition": "hot|cold|dry|humid|bad"}}
  ],
  "needs_control": true/false,
  "summary": "ê°„ë‹¨í•œ ìš”ì•½"
}}

**ë¶„ë¥˜ ê¸°ì¤€:**
- environment_complaint: "ë¥ë‹¤", "ì¶¥ë‹¤", "ê±´ì¡°í•˜ë‹¤", "ìŠµí•˜ë‹¤", "ê³µê¸° ë‚˜ì˜ë‹¤", "ë‹µë‹µí•˜ë‹¤" ë“± í™˜ê²½ ë¶ˆí¸ í‘œí˜„
  â†’ ì´ ê²½ìš° **ë°˜ë“œì‹œ needs_control: true**
- appliance_request: "ì—ì–´ì»¨ ì¼œì¤˜", "ë¶ˆ ì¼œì¤˜" ë“± ì§ì ‘ì ì¸ ê°€ì „ ì œì–´ ìš”ì²­
  â†’ ì´ ê²½ìš°ë„ **ë°˜ë“œì‹œ needs_control: true**
- general_chat: ì¼ë°˜ ëŒ€í™” ("ì•ˆë…•", "ê³ ë§ˆì›Œ", "ë‚ ì”¨ ì–´ë•Œ?" ë“±)
  â†’ ì´ ê²½ìš°ë§Œ needs_control: false

**condition ê°’:**
- temperature: "hot" (ë¥ë‹¤/ë”ì›Œ) / "cold" (ì¶¥ë‹¤/ì¶”ì›Œ)
- humidity: "dry" (ê±´ì¡°í•˜ë‹¤) / "humid" (ìŠµí•˜ë‹¤)
- air_quality: "bad" (ê³µê¸° ë‚˜ì˜ë‹¤/ë‹µë‹µí•˜ë‹¤)

**ì¤‘ìš”:** ì‚¬ìš©ìê°€ í™˜ê²½ì— ëŒ€í•œ ë¶ˆí¸í•¨ì„ í‘œí˜„í•˜ë©´ ë¬´ì¡°ê±´ needs_control: trueì…ë‹ˆë‹¤!
"""

            if context:
                prompt += f"\n\n**í˜„ì¬ í™˜ê²½:**\n{json.dumps(context, ensure_ascii=False, indent=2)}"

            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì˜ë„ íŒŒì‹± ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            result = json.loads(content)

            logger.info(f"âœ… Intent parsed: {result.get('intent_type')}, needs_control={result.get('needs_control')}")
            return result

        except Exception as e:
            logger.error(f"âŒ Intent parsing error: {str(e)}")
            return {
                "intent_type": "general_chat",
                "issues": [],
                "needs_control": False,
                "summary": "íŒŒì‹± ì‹¤íŒ¨"
            }

    async def generate_appliance_suggestion(
        self,
        appliances: List[Dict[str, Any]],
        weather: Dict[str, Any],
        fatigue_level: int,
        user_message: str,
        persona: Optional[Dict] = None,
        appliance_states: Optional[List[Dict]] = None,
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """
        ê°€ì „ ì œì–´ ì œì•ˆ ë©”ì‹œì§€ ìƒì„± (ì‹œë‚˜ë¦¬ì˜¤ 2ìš©)

        Args:
            appliances: ì œì–´í•  ê°€ì „ ëª©ë¡
            weather: ë‚ ì”¨ ë°ì´í„°
            fatigue_level: í”¼ë¡œë„ ë ˆë²¨
            user_message: ì‚¬ìš©ì ì›ë³¸ ë©”ì‹œì§€
            persona: í˜ë¥´ì†Œë‚˜ ì •ë³´ (nickname, description)
            appliance_states: í˜„ì¬ ê°€ì „ ìƒíƒœ ë¦¬ìŠ¤íŠ¸
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬

        Returns:
            ì œì•ˆ ë©”ì‹œì§€ (ì˜ˆ: "í˜„ì¬ ì˜¨ë„ê°€ 28ë„ë¡œ ë†’ê³ , í”¼ë¡œë„ê°€ 3ì´ì—ìš”. ì—ì–´ì»¨ì„ 23ë„ë¡œ ì¼œê³ , ê³µê¸°ì²­ì •ê¸°ë„ ì¼¤ê¹Œìš”?")
        """
        try:
            appliance_info = []
            for app in appliances:
                info = f"{app['appliance_type']}: {app['settings']}"
                if app.get('reason'):
                    info += f" ({app['reason']})"
                appliance_info.append(info)

            prompt = f"""ì‚¬ìš©ìê°€ "{user_message}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.

**í˜„ì¬ ìƒí™©:**
- ì˜¨ë„: {weather.get('temperature')}Â°C
- ìŠµë„: {weather.get('humidity')}%
- ë¯¸ì„¸ë¨¼ì§€: {weather.get('pm10')} ã/ã¥
- í”¼ë¡œë„ ë ˆë²¨: {fatigue_level} (1:ì¢‹ìŒ, 2:ë³´í†µ, 3:ë‚˜ì¨, 4:ë§¤ìš°ë‚˜ì¨)

**ì¶”ì²œ ê°€ì „ ì œì–´:**
{chr(10).join(appliance_info)}
"""

            # í˜„ì¬ ê°€ì „ ìƒíƒœ ì¶”ê°€
            if appliance_states:
                prompt += "\n**í˜„ì¬ ê°€ì „ ìƒíƒœ:**\n"
                for app in appliance_states:
                    status = "ì¼œì§" if app.get("is_on") else "êº¼ì§"
                    prompt += f"- {app['appliance_type']}: {status}"
                    if app.get("is_on") and app.get("current_settings"):
                        prompt += f" (ì„¤ì •: {app['current_settings']})"
                    prompt += "\n"

            prompt += """
ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ê°€ì „ ì œì–´ë¥¼ ì œì•ˆí•˜ëŠ” ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ì„¸ìš”.
í˜•ì‹: "í˜„ì¬ [ë‚ ì”¨ ì„¤ëª…]. [ê°€ì „ ì œì–´ ì œì•ˆ]í• ê¹Œìš”?"
ë°˜ë“œì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (JSON ì•„ë‹˜).
"""

            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (í˜ë¥´ì†Œë‚˜ ì ìš©)
            system_prompt = self._build_system_prompt(persona)

            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=200
            )

            suggestion = response.choices[0].message.content.strip()
            logger.info(f"âœ… Suggestion generated: {suggestion[:50]}...")
            return suggestion

        except Exception as e:
            logger.error(f"âŒ Suggestion generation error: {str(e)}")
            # Fallback
            appliance_names = [a["appliance_type"] for a in appliances]
            return f"í˜„ì¬ ë‚ ì”¨ì™€ í”¼ë¡œë„ë¥¼ ê³ ë ¤í•´ì„œ {', '.join(appliance_names)}ì„(ë¥¼) ì¼œë“œë¦´ê¹Œìš”?"

    async def detect_modification(
        self,
        original_plan: Dict[str, Any],
        user_response: str
    ) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ìˆ˜ì • ì‚¬í•­ ê°ì§€ (ì‹œë‚˜ë¦¬ì˜¤ 2ìš©)

        ì˜ˆ:
        - "ì—ì–´ì»¨ì€ 24ë„ë¡œ" â†’ {"ì—ì–´ì»¨": {"target_temp_c": 24}}
        - "ê³µê¸°ì²­ì •ê¸°ëŠ” ë„ê³ " â†’ {"ê³µê¸°ì²­ì •ê¸°": {"action": "off"}}
        - "ê·¸ëŒ€ë¡œ í•´ì¤˜" â†’ {}

        Args:
            original_plan: ì›ë˜ ì œì•ˆí–ˆë˜ ê°€ì „ ì œì–´ ê³„íš
            user_response: ì‚¬ìš©ì ì‘ë‹µ

        Returns:
            {
                "has_modification": true/false,
                "modifications": {
                    "ì—ì–´ì»¨": {"target_temp_c": 24},
                    "ê³µê¸°ì²­ì •ê¸°": {"action": "off"}
                },
                "approved": true/false
            }
        """
        try:
            appliances_str = json.dumps([
                {
                    "appliance_type": a["appliance_type"],
                    "settings": a.get("settings", {})
                }
                for a in original_plan.get("appliances", [])
            ], ensure_ascii=False, indent=2)

            prompt = f"""ì›ë˜ ì œì•ˆ:
{appliances_str}

ì‚¬ìš©ì ì‘ë‹µ: "{user_response}"

ì‚¬ìš©ìê°€ ìˆ˜ì •ì„ ìš”ì²­í–ˆëŠ”ì§€, ê·¸ëŒ€ë¡œ ìŠ¹ì¸í–ˆëŠ”ì§€, ê±°ë¶€í–ˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
  "approved": true/false,
  "has_modification": true/false,
  "modifications": {{
    "ê°€ì „ì¢…ë¥˜": {{"ì„¤ì •í‚¤": ê°’}}
  }},
  "reason": "íŒë‹¨ ì´ìœ "
}}

**ì˜ˆì‹œ:**
- "ì¢‹ì•„", "ê·¸ë˜", "ì‘" â†’ approved: true, has_modification: false
- "ì—ì–´ì»¨ì€ 24ë„ë¡œ" â†’ approved: true, has_modification: true, modifications: {{"ì—ì–´ì»¨": {{"target_temp_c": 24}}}}
- "ê³µê¸°ì²­ì •ê¸°ëŠ” ë„ê³ " â†’ approved: true, has_modification: true, modifications: {{"ê³µê¸°ì²­ì •ê¸°": {{"action": "off"}}}}
- "ì•„ë‹ˆì•¼", "ê´œì°®ì•„" â†’ approved: false
"""

            response = await client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì‚¬ìš©ì ì‘ë‹µ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                response_format={"type": "json_object"}
            )

            content = response.choices[0].message.content
            result = json.loads(content)

            logger.info(f"âœ… Modification detected: approved={result.get('approved')}, has_mod={result.get('has_modification')}")
            return result

        except Exception as e:
            logger.error(f"âŒ Modification detection error: {str(e)}")
            return {
                "approved": True,
                "has_modification": False,
                "modifications": {},
                "reason": "íŒŒì‹± ì‹¤íŒ¨ - ì›ë˜ ê³„íš ì‹¤í–‰"
            }


class MemoryService:
    """ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬"""

    def __init__(self):
        # ì‹¤ì œë¡œëŠ” DBì— ì €ì¥í•´ì•¼ í•¨
        from collections import OrderedDict
        from datetime import datetime, timedelta

        self.short_term_memory: OrderedDict[str, List[Dict]] = OrderedDict()
        self.long_term_memory: OrderedDict[str, Dict] = OrderedDict()

        # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
        self.MAX_USERS = 200  # ìµœëŒ€ ì‚¬ìš©ì ìˆ˜
        self.MAX_MESSAGES_PER_USER = 50  # ì‚¬ìš©ìë‹¹ ìµœëŒ€ ë©”ì‹œì§€ ìˆ˜
        self.MEMORY_TIMEOUT = timedelta(hours=6)  # 6ì‹œê°„ ë™ì•ˆ ì ‘ê·¼ ì—†ìœ¼ë©´ ì‚­ì œ
        self.last_access: Dict[str, datetime] = {}

    def _cleanup_old_memories(self):
        """ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì •ë¦¬"""
        from datetime import datetime
        now = datetime.now()
        to_delete = []

        for user_id in list(self.short_term_memory.keys()):
            last_time = self.last_access.get(user_id, now)
            if now - last_time > self.MEMORY_TIMEOUT:
                to_delete.append(user_id)

        for user_id in to_delete:
            self.short_term_memory.pop(user_id, None)
            self.long_term_memory.pop(user_id, None)
            self.last_access.pop(user_id, None)
            logger.info(f"ğŸ—‘ï¸ Cleaned up memory for user: {user_id}")

        # ìµœëŒ€ ì‚¬ìš©ì ìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ê²ƒë¶€í„° ì‚­ì œ (LRU)
        while len(self.short_term_memory) > self.MAX_USERS:
            oldest_user = next(iter(self.short_term_memory))
            self.short_term_memory.pop(oldest_user, None)
            self.long_term_memory.pop(oldest_user, None)
            self.last_access.pop(oldest_user, None)
            logger.info(f"ğŸ—‘ï¸ Evicted memory (max limit): {oldest_user}")

    def add_message(self, user_id: str, role: str, content: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
        from datetime import datetime
        import random

        # 10% í™•ë¥ ë¡œ ì •ë¦¬ ì‹¤í–‰
        if random.random() < 0.1:
            self._cleanup_old_memories()

        if user_id not in self.short_term_memory:
            self.short_term_memory[user_id] = []

        self.short_term_memory[user_id].append({
            "role": role,
            "content": content
        })

        # ìµœê·¼ ë©”ì‹œì§€ë§Œ ìœ ì§€
        if len(self.short_term_memory[user_id]) > self.MAX_MESSAGES_PER_USER:
            self.short_term_memory[user_id] = self.short_term_memory[user_id][-self.MAX_MESSAGES_PER_USER:]

        # ì ‘ê·¼ ì‹œê°„ ê°±ì‹  (LRU)
        self.last_access[user_id] = datetime.now()
        # OrderedDictì—ì„œ ìµœì‹  í•­ëª©ìœ¼ë¡œ ì´ë™
        if user_id in self.short_term_memory:
            self.short_term_memory.move_to_end(user_id)

    def get_history(self, user_id: str, limit: int = 10) -> List[Dict]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        from datetime import datetime

        if user_id not in self.short_term_memory:
            return []

        # ì ‘ê·¼ ì‹œê°„ ê°±ì‹ 
        self.last_access[user_id] = datetime.now()
        self.short_term_memory.move_to_end(user_id)

        return self.short_term_memory[user_id][-limit:]

    def update_long_term_memory(self, user_id: str, key: str, value: Any):
        """ì¥ê¸° ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸"""
        from datetime import datetime

        if user_id not in self.long_term_memory:
            self.long_term_memory[user_id] = {}

        self.long_term_memory[user_id][key] = value

        # ì ‘ê·¼ ì‹œê°„ ê°±ì‹ 
        self.last_access[user_id] = datetime.now()
        if user_id in self.long_term_memory:
            self.long_term_memory.move_to_end(user_id)

    def get_long_term_memory(self, user_id: str) -> Dict:
        """ì¥ê¸° ë©”ëª¨ë¦¬ ì¡°íšŒ"""
        from datetime import datetime

        if user_id in self.last_access:
            self.last_access[user_id] = datetime.now()
        if user_id in self.long_term_memory:
            self.long_term_memory.move_to_end(user_id)

        return self.long_term_memory.get(user_id, {})


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
llm_service = LLMService()
memory_service = MemoryService()
